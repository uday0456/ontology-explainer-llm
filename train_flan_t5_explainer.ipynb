{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Fine-Tuning FLAN-T5 for Ontology Explanation Generation"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n", "from datasets import load_dataset, Dataset\n", "import json\n\n", "# Load training data\n", "with open('../data/training_pairs.json') as f:\n", "    data = json.load(f)\n", "dataset = Dataset.from_dict({\"input\": [data[\"input\"]], \"output\": [data[\"output\"]]})\n\n", "# Load model and tokenizer\n", "model_name = \"google/flan-t5-base\"\n", "tokenizer = AutoTokenizer.from_pretrained(model_name)\n", "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n\n", "# Tokenize\n", "def preprocess(examples):\n", "    inputs = tokenizer(examples['input'], padding='max_length', truncation=True, max_length=128)\n", "    outputs = tokenizer(examples['output'], padding='max_length', truncation=True, max_length=128)\n", "    inputs['labels'] = outputs['input_ids']\n", "    return inputs\n", "tokenized = dataset.map(preprocess)\n\n", "# Training arguments\n", "training_args = Seq2SeqTrainingArguments(\n", "    output_dir=\"../model/flan-t5-finetuned\",\n", "    per_device_train_batch_size=2,\n", "    num_train_epochs=1,\n", "    logging_dir=\"../logs\",\n", "    logging_steps=10,\n", "    save_total_limit=1,\n", ")\n\n", "# Trainer\n", "trainer = Seq2SeqTrainer(\n", "    model=model,\n", "    args=training_args,\n", "    train_dataset=tokenized,\n", "    tokenizer=tokenizer,\n", "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model)\n", ")\n", "trainer.train()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.9"}}, "nbformat": 4, "nbformat_minor": 5}